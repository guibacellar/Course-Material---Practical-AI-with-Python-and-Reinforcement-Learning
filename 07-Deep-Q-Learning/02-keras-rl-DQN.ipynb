{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "south-metabolism",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'><img src='../Pierian_Data_Logo.png'/></a>\n",
    "___\n",
    "<center><em>Copyright by Pierian Data Inc.</em></center>\n",
    "<center><em>For more information, visit us at <a href='http://www.pieriandata.com'>www.pieriandata.com</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-satisfaction",
   "metadata": {},
   "source": [
    "# Keras-RL DQN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-firewall",
   "metadata": {},
   "source": [
    "## Introduction to keras-rl(2)\n",
    "\n",
    "In this notebook we will create our first Reinforcement Learning agent via keras-rl together,\n",
    "based on a simple task from open-ai gym, namely the *Cartpole Example*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-accident",
   "metadata": {},
   "source": [
    "At first we will import all necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # to reduce the game speed when playing manually\n",
    "\n",
    "import gym  # Contains the game we want to play\n",
    "from pyglet.window import key  # for manual playing\n",
    "\n",
    "# import necessary blocks from keras to build the Deep Learning backbone of our agent\n",
    "from tensorflow.keras.models import Sequential  # To compose multiple Layers\n",
    "from tensorflow.keras.layers import Dense  # Fully-Connected layer\n",
    "from tensorflow.keras.layers import Activation  # Activation functions\n",
    "from tensorflow.keras.layers import Flatten  # Flatten function\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam  # Adam optimizer\n",
    "\n",
    "# Now the keras-rl2 agent. Dont get confused as it is only called rl and not keras-rl\n",
    "\n",
    "from rl.agents.dqn import DQNAgent  # Use the basic Deep-Q-Network agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-warehouse",
   "metadata": {},
   "source": [
    "Now we will create the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/56904270/difference-between-openai-gym-environments-cartpole-v0-and-cartpole-v1\n",
    "env_name = ENV_NAME = 'CartPole-v0'  # https://gym.openai.com/envs/CartPole-v1/\n",
    "env = gym.make(env_name)  # create the environment\n",
    "nb_actions = env.action_space.n  # get the number of possible actions\n",
    "print(nb_actions)  # Cartpole has only two possible actions: Either move left or right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-admission",
   "metadata": {},
   "source": [
    "Lets watch how the game looks when chosing random actions, or to be precise randomly move left and right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()  # reset the environment to the initial state\n",
    "for _ in range(200):  # play for max 200 iterations\n",
    "    env.render(mode=\"human\")  # render the current game state on your screen\n",
    "    random_action = env.action_space.sample()  # chose a random action\n",
    "    env.step(random_action)  # execute that action\n",
    "env.close()  # close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-necessity",
   "metadata": {},
   "source": [
    "Now it is time that you try your luck! Try it out by using the left and right arrow key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 0\n",
    "def key_press(k, mod):\n",
    "    '''\n",
    "    This function gets the key press for gym\n",
    "    '''\n",
    "    global action\n",
    "    if k == key.LEFT:\n",
    "        action = 0\n",
    "    if k == key.RIGHT:\n",
    "        action = 1\n",
    "\n",
    "env.reset()\n",
    "rewards = 0\n",
    "for _ in range(1000):\n",
    "    env.render(mode=\"human\")\n",
    "    env.viewer.window.on_key_press = key_press  # update the key press\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    rewards+=1\n",
    "    if done:\n",
    "        print(f\"You got {rewards} points!\")\n",
    "        break\n",
    "    time.sleep(0.1)  # reduce speed a little bit\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-thinking",
   "metadata": {},
   "source": [
    "Let us build a Deep Neural Network and try if it can beat our score\n",
    "\n",
    "We use the same simple model with 2 hidden layers with 16 and 32 neurons each followed by relu activation\n",
    "\n",
    "The output layer has 2 nodes, one for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-spirituality",
   "metadata": {},
   "source": [
    "Lets create the DQN agent from keras-rl\n",
    "For this setting, the agent takes the following parameters:\n",
    "\n",
    "1. model = The model\n",
    "2. nb_actions = The number of actions (2 in this case)\n",
    "3. memory = The action replay memory. You can choose between the *SequentialMemory()* and *EpisodeParameterMemory() which is only used for one RL agent called CEM*\n",
    "4. nb_steps_warmup = How many iterations without training - Used to fill the memory\n",
    "5. target_model_update = When do we update the target model?\n",
    "6. Action Selection policy. You can choose between a *LinearAnnealedPolicy()*, *SoftmaxPolicy()*, *EpsGreedyQPolicy()*, *GreedyQPolicy()*, *GreedyQPolicy()*, *MaxBoltzmannQPolicy()* and *BoltzmannGumbelQPolicy()*. We use all of them during the next notebooks but feel free to try them out and inspect which works best here\n",
    "\n",
    "There are some more parameters, you can pass to the DQN Agent. Feel free to explore them, but we will also take a look at them together in the remaining notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-pleasure",
   "metadata": {},
   "source": [
    "Here we initialize the circular buffer with a limit of 20000 and a window length of 1.\n",
    "The window length describes the number of subsequent actions stored for a state.\n",
    "This will be demonstrated in the next lecture, when we start dealing with images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory  # Sequential Memory for storing observations ( optimized circular buffer)\n",
    "\n",
    "memory = SequentialMemory(limit=20000, window_length=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-village",
   "metadata": {},
   "source": [
    "Then we define the Action Selection Policy: <br />\n",
    "We use *LinearAnnealedPolicy* in order to perform the epsilon greedy strategy with decaying epsilon. <br />\n",
    "*LinearAnnealedPolicy* accepts an action selection policy, its maximal and minimal values and a step number in order to create a dynimal policy. <br/>\n",
    "The minimal value epsilon can reach during training is 0.1.<br />\n",
    "For evaluation (e.g running the agent) it is fixed to 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearAnnealedPolicy allows to decay the epsilon for the epsilon greedy strategy\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps',\n",
    "                              value_max=1.,\n",
    "                              value_min=.1,\n",
    "                              value_test=.05,\n",
    "                              nb_steps=20000) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-audience",
   "metadata": {},
   "source": [
    "Now we create the DQN Agent based on the defined model (**model**), the possible actions (**nb_actions**) (left and right in this case), the circular buffer (**memory**), the burnin or warmup phase (**10**), how often the target model gets updated (**100**) and the policy (**policy**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=100, policy=policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-habitat",
   "metadata": {},
   "source": [
    "Finally we compile our model with the Adam optimizer and a learning rate of 0.001.<br />\n",
    "We log the Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use learning_rate instead of lr if you get warning\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-rabbit",
   "metadata": {},
   "source": [
    "Now we run the training for 20000 steps. You can change visualize=True if you want to watch your model learning.\n",
    "Keep in mind that this increases the running time\n",
    "The training time is around 5 min so grep your favorite beverage and stay tuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-championship",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-alcohol",
   "metadata": {},
   "source": [
    "Wow! After only some minutes of training, we achieve great results!\n",
    "The reason for this is, that keras-rl has implemented many optimization strategies (e.g the optimized replay buffer) which lead to a much faster convergence than our DQN implemented by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights(f'dqn_{env_name}_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-nirvana",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
