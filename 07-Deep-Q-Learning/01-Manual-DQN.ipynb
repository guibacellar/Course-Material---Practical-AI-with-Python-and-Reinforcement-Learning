{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surrounded-wings",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'><img src='../Pierian_Data_Logo.png'/></a>\n",
    "___\n",
    "<center><em>Copyright by Pierian Data Inc.</em></center>\n",
    "<center><em>For more information, visit us at <a href='http://www.pieriandata.com'>www.pieriandata.com</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-alberta",
   "metadata": {},
   "source": [
    "\n",
    "# Manually Creating a DQN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-porcelain",
   "metadata": {},
   "source": [
    "## Deep-Q-Learning\n",
    "In this notebook we will create our first Deep Reeinforcement Learning model, called Deep-Q-Network (DQN)\n",
    "We are again using a simple environment from openai gym. <br />\n",
    "However, you will soon see the enormous gain we will get by switching from standard Q-Learning to Deep Q Learning.\n",
    "\n",
    "In this notebook we again take a look at the CartPole problem (https://gym.openai.com/envs/CartPole-v1/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-destruction",
   "metadata": {},
   "source": [
    "Let us start by importing the necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239dd36",
   "metadata": {},
   "source": [
    "# Part 0: Imports\n",
    "\n",
    "Notice how we're importing the TF libraries here at the top together, in some rare instances, if you import them later on, you get strange bugs, so best just to import everything from Tensorflow here at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hybrid-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gym  # Contains the game we want to play\n",
    "from tensorflow.keras.models import Sequential  # To compose multiple Layers\n",
    "from tensorflow.keras.layers import Dense  # Fully-Connected layer\n",
    "from tensorflow.keras.layers import Activation  # Activation functions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df24c33",
   "metadata": {},
   "source": [
    "# Part 1: The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fifty-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)  # create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-exploration",
   "metadata": {},
   "source": [
    "Remember, the goal of the CartPole challenge was to balance the stick upright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "southeast-mechanism",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\marcial\\anaconda_new\\envs\\rl_recording\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env.reset()  # reset the environment to the initial state\n",
    "for _ in range(200):  # play for max 200 iterations\n",
    "    env.render(mode=\"human\")  # render the current game state on your screen\n",
    "    random_action = env.action_space.sample()  # chose a random action\n",
    "    env.step(random_action)  # execute that action\n",
    "env.close()  # close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf18acd",
   "metadata": {},
   "source": [
    "# Part 2: The Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-unknown",
   "metadata": {},
   "source": [
    "### Let us build our first Neural Network\n",
    "To build our network, we first need to find out how many actions and observation our environment has.\n",
    "We can either get those information from the source code (https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) or via the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "widespread-oxide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 possible actions and 4 observations\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape[0]  # You can use this command to get the number of observations\n",
    "print(f\"There are {num_actions} possible actions and {num_observations} observations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-alaska",
   "metadata": {},
   "source": [
    "So our network needs to have an input dimension of 4 and an output dimension of 2.\n",
    "In between we are free to chose.\n",
    "\n",
    "Let's just say we want to use a four layer architecture:\n",
    "\n",
    "\n",
    "1. The first layer has 16 neurons\n",
    "2. The second layer has 32 neurons\n",
    "4. The fourth layer (output layer) has 2 neurons\n",
    "\n",
    "This yields 690 parameters\n",
    "$$ \\text{4 observations} * 16 (\\text{neurons}) + 16 (\\text{bias}) + (16*32) + 32 + (32*2)+2 = 690$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "insured-words",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1, 16)             80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 32)             544       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 2)              66        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1, 2)              0         \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(16, input_shape=(1, num_observations)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-sitting",
   "metadata": {},
   "source": [
    "Now we have our model which takes an observation as input and outputs a value for each action.\n",
    "The higher the value, the more likely that this value is a suitable action for the current observation\n",
    "\n",
    "As stated in the lecture, Deep-Q-Learning works better when using a target network.\n",
    "So let's just copy the above network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "advanced-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"34.ckt\")\n",
    "target_model = clone_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-terrorism",
   "metadata": {},
   "source": [
    "Now it is time to define our hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7991ba",
   "metadata": {},
   "source": [
    "# Part 3: Hyperparameters and Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interim-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "epsilon = 1.0\n",
    "EPSILON_REDUCE = 0.995  # is multiplied with epsilon each epoch to reduce it\n",
    "LEARNING_RATE = 0.001 #NOT THE SAME AS ALPHA FROM Q-LEARNING FROM BEFORE!!\n",
    "GAMMA = 0.95\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-simple",
   "metadata": {},
   "source": [
    "Let us use the epsilon greedy action selection function once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coordinate-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(model, epsilon, observation):\n",
    "    if np.random.random() > epsilon:\n",
    "        prediction = model.predict(observation)  # perform the prediction on the observation\n",
    "        action = np.argmax(prediction)  # Chose the action with the higher value\n",
    "    else:\n",
    "        action = np.random.randint(0, env.action_space.n)  # Else use random action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-local",
   "metadata": {},
   "source": [
    "As shown in the lecture, we need a replay buffer.\n",
    "We can use the **deque** data structure for this, which already implements the circular behavior.\n",
    "\n",
    "The *maxlen* argument specifies the number of elements the buffer can store between he overwrites them at the beginning\n",
    "\n",
    "The following cell shows an example usage of the deque function. You can see, that in the first example all values fit into the deque, so nothing is overwritten. \n",
    "\n",
    "In the second example, the deque is printed in each iteration. It can hold all values in the first five iterations but then needs to delete the oldest value in the deque to make room for the new value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brilliant-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([0, 1, 2, 3, 4], maxlen=5)\n",
      "---------------------\n",
      "deque([0], maxlen=5)\n",
      "deque([0, 1], maxlen=5)\n",
      "deque([0, 1, 2], maxlen=5)\n",
      "deque([0, 1, 2, 3], maxlen=5)\n",
      "deque([0, 1, 2, 3, 4], maxlen=5)\n",
      "deque([1, 2, 3, 4, 5], maxlen=5)\n",
      "deque([2, 3, 4, 5, 6], maxlen=5)\n",
      "deque([3, 4, 5, 6, 7], maxlen=5)\n",
      "deque([4, 5, 6, 7, 8], maxlen=5)\n",
      "deque([5, 6, 7, 8, 9], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "### deque examples\n",
    "deque_1 = deque(maxlen=5)\n",
    "for i in range(5):  # all values fit into the deque, no overwriting\n",
    "    deque_1.append(i)\n",
    "print(deque_1)\n",
    "print(\"---------------------\")\n",
    "deque_2 = deque(maxlen=5)\n",
    "\n",
    "# after the first 5 values are stored, it needs to overwrite the oldest value to store the new one\n",
    "for i in range(10):  \n",
    "    deque_2.append(i)\n",
    "    print(deque_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-august",
   "metadata": {},
   "source": [
    "Let's say we allow our replay buffer a maximum size of 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thrown-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=20000)\n",
    "update_target_model = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-silicon",
   "metadata": {},
   "source": [
    "As mentioned in the lecture, action replaying is crucial for Deep Q-Learning. <br />\n",
    "The following cell implements one version of the action replay algorithm. <br />\n",
    "It uses the zip statement paired with the * (Unpacking Argument Lists) operator to create batches from the samples for efficient prediction and training.<br />\n",
    "The zip statement returns all corresponding pairs from each entry. <br />\n",
    "It might look confusing but the following example should clarify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "constant-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 7) (2, 5, 8) (3, 6, 9)\n"
     ]
    }
   ],
   "source": [
    "test_tuple = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n",
    "zipped_list = list(zip(*test_tuple))\n",
    "a, b, c = zipped_list\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-vegetation",
   "metadata": {},
   "source": [
    "Now it's time to write the replay function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scheduled-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_buffer, batch_size, model, target_model):\n",
    "    \n",
    "    # As long as the buffer has not enough elements we do nothing\n",
    "    if len(replay_buffer) < batch_size: \n",
    "        return\n",
    "    \n",
    "    # Take a random sample from the buffer with size batch_size\n",
    "    samples = random.sample(replay_buffer, batch_size)  \n",
    "    \n",
    "    # to store the targets predicted by the target network for training\n",
    "    target_batch = []  \n",
    "    \n",
    "    # Efficient way to handle the sample by using the zip functionality\n",
    "    zipped_samples = list(zip(*samples))  \n",
    "    states, actions, rewards, new_states, dones = zipped_samples  \n",
    "    \n",
    "    # Predict targets for all states from the sample\n",
    "    targets = target_model.predict(np.array(states))\n",
    "    \n",
    "    # Predict Q-Values for all new states from the sample\n",
    "    q_values = model.predict(np.array(new_states))  \n",
    "    \n",
    "    # Now we loop over all predicted values to compute the actual targets\n",
    "    for i in range(batch_size):  \n",
    "        \n",
    "        # Take the maximum Q-Value for each sample\n",
    "        q_value = max(q_values[i][0])  \n",
    "        \n",
    "        # Store the ith target in order to update it according to the formula\n",
    "        target = targets[i].copy()  \n",
    "        if dones[i]:\n",
    "            target[0][actions[i]] = rewards[i]\n",
    "        else:\n",
    "            target[0][actions[i]] = rewards[i] + q_value * GAMMA\n",
    "        target_batch.append(target)\n",
    "\n",
    "    # Fit the model based on the states and the updated targets for 1 epoch\n",
    "    model.fit(np.array(states), np.array(target_batch), epochs=1, verbose=0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-testing",
   "metadata": {},
   "source": [
    "We need to update our target network every once in a while. <br />\n",
    "Keras provides the *set_weights()* and *get_weights()* methods which do the work for us, so we only need to check whether we hit an update epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stainless-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_handler(epoch, update_target_model, model, target_model):\n",
    "    if epoch > 0 and epoch % update_target_model == 0:\n",
    "        target_model.set_weights(model.get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4755d07",
   "metadata": {},
   "source": [
    "# Part 4: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-michael",
   "metadata": {},
   "source": [
    "Now it is time to write the training loop! <br />\n",
    "First we compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sharing-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-mineral",
   "metadata": {},
   "source": [
    "Then we perform the training routine. <br />\n",
    "This might take some time, so make sure to grab your favorite beverage and watch your model learn. <br />\n",
    "Feel free to use our provided chekpoints as a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-interface",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Points reached: 18 - epsilon: 0.995 - Best: 18\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1, 4), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 4).\n",
      "25: Points reached: 13 - epsilon: 0.8778091417340573 - Best: 53\n",
      "50: Points reached: 25 - epsilon: 0.7744209942832988 - Best: 80\n",
      "75: Points reached: 32 - epsilon: 0.6832098777212641 - Best: 80\n",
      "100: Points reached: 79 - epsilon: 0.6027415843082742 - Best: 118\n",
      "125: Points reached: 39 - epsilon: 0.531750826943791 - Best: 151\n",
      "150: Points reached: 24 - epsilon: 0.46912134373457726 - Best: 191\n",
      "175: Points reached: 160 - epsilon: 0.41386834584198684 - Best: 191\n",
      "200: Points reached: 148 - epsilon: 0.36512303261753626 - Best: 287\n",
      "225: Points reached: 130 - epsilon: 0.322118930542046 - Best: 287\n",
      "250: Points reached: 205 - epsilon: 0.28417984116121187 - Best: 287\n",
      "275: Points reached: 139 - epsilon: 0.2507092085103961 - Best: 299\n",
      "300: Points reached: 169 - epsilon: 0.2211807388415433 - Best: 351\n",
      "325: Points reached: 184 - epsilon: 0.19513012515638165 - Best: 351\n",
      "350: Points reached: 230 - epsilon: 0.17214774642209296 - Best: 351\n",
      "375: Points reached: 175 - epsilon: 0.1518722266715875 - Best: 380\n",
      "400: Points reached: 143 - epsilon: 0.13398475271138335 - Best: 380\n",
      "425: Points reached: 152 - epsilon: 0.11820406108847166 - Best: 380\n",
      "450: Points reached: 176 - epsilon: 0.1042820154910064 - Best: 380\n",
      "475: Points reached: 161 - epsilon: 0.09199970504166631 - Best: 380\n",
      "500: Points reached: 185 - epsilon: 0.0811640021330769 - Best: 380\n",
      "525: Points reached: 158 - epsilon: 0.0716045256805401 - Best: 380\n",
      "550: Points reached: 139 - epsilon: 0.06317096204211972 - Best: 380\n",
      "575: Points reached: 148 - epsilon: 0.05573070148010834 - Best: 380\n",
      "600: Points reached: 156 - epsilon: 0.04916675299948831 - Best: 380\n",
      "625: Points reached: 141 - epsilon: 0.043375904776212296 - Best: 380\n",
      "650: Points reached: 159 - epsilon: 0.03826710124979409 - Best: 380\n",
      "675: Points reached: 167 - epsilon: 0.033760011361539714 - Best: 380\n",
      "700: Points reached: 135 - epsilon: 0.029783765425331846 - Best: 380\n",
      "725: Points reached: 169 - epsilon: 0.026275840769466357 - Best: 380\n",
      "750: Points reached: 157 - epsilon: 0.023181078627322618 - Best: 380\n",
      "775: Points reached: 146 - epsilon: 0.020450816818411825 - Best: 380\n"
     ]
    }
   ],
   "source": [
    "best_so_far = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    observation = env.reset()  # Get inital state\n",
    "    \n",
    "    # Keras expects the input to be of shape [1, X] thus we have to reshape\n",
    "    observation = observation.reshape([1, 4])  \n",
    "    done = False  \n",
    "    \n",
    "    points = 0\n",
    "    while not done:  # as long current run is active\n",
    "        \n",
    "        # Select action acc. to strategy\n",
    "        action = epsilon_greedy_action_selection(model, epsilon, observation)\n",
    "        \n",
    "        # Perform action and get next state\n",
    "        next_observation, reward, done, info = env.step(action)  \n",
    "        next_observation = next_observation.reshape([1, 4])  # Reshape!!\n",
    "        replay_buffer.append((observation, action, reward, next_observation, done))  # Update the replay buffer\n",
    "        observation = next_observation  # update the observation\n",
    "        points+=1\n",
    "\n",
    "        # Most important step! Training the model by replaying\n",
    "        replay(replay_buffer, 32, model, target_model)\n",
    "\n",
    "    \n",
    "    epsilon *= EPSILON_REDUCE  # Reduce epsilon\n",
    "    \n",
    "    # Check if we need to update the target model\n",
    "    update_model_handler(epoch, update_target_model, model, target_model)\n",
    "    \n",
    "    if points > best_so_far:\n",
    "        best_so_far = points\n",
    "    if epoch %25 == 0:\n",
    "        print(f\"{epoch}: Points reached: {points} - epsilon: {epsilon} - Best: {best_so_far}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e969bb",
   "metadata": {},
   "source": [
    "# Part 5: Using Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "for counter in range(300):\n",
    "    env.render()\n",
    "    \n",
    "    # TODO: Get discretized observation\n",
    "    action = np.argmax(model.predict(observation.reshape([1,4])))\n",
    "    \n",
    "    # TODO: Perform the action \n",
    "    observation, reward, done, info = env.step(action) # Finally perform the action\n",
    "    \n",
    "    if done:\n",
    "        print(f\"done\")\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
